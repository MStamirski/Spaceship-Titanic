{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNiPnmzqa7EiKU6VIODp0MD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MStamirski/Spaceship-Titanic/blob/main/Model_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features datasets"
      ],
      "metadata": {
        "id": "Q138noD8d-zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "%cd \"/content/drive/MyDrive/Colab_Notebooks/SDA_upskill/Spaceship\""
      ],
      "metadata": {
        "id": "P4H_6iE_du7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install import-ipynb"
      ],
      "metadata": {
        "id": "awVb5jdEdiTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import import_ipynb"
      ],
      "metadata": {
        "id": "XYnQGhfydlLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from FeaturesEngineering import get_features, categories_one_hot_encoding, categories_target_encoding, categories_leave_one_out_encoding"
      ],
      "metadata": {
        "id": "4Y8YBEFddnp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = get_features('train')\n",
        "df_ohe = categories_one_hot_encoding(df)\n",
        "df_te = categories_target_encoding(df)\n",
        "df_looe = categories_leave_one_out_encoding(df)"
      ],
      "metadata": {
        "id": "2aSTdoZXeDHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_subsets(dataset):\n",
        "  X = dataset.drop(columns=['PassengerId', 'Transported'])\n",
        "  y = dataset['Transported']\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "QfSEnwkZHU5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression in PyTorch"
      ],
      "metadata": {
        "id": "icgzte-ceHWx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evY53SATZ3Dv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression(torch.nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(LogisticRegression, self).__init__()\n",
        "    self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "  def forward(self, x):\n",
        "    outputs = torch.sigmoid(self.linear(x))\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "JRRkk9rJc875"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer):\n",
        "\n",
        "  model.train()\n",
        "  loss_val, correct = 0.0, 0\n",
        "\n",
        "  for Xt, yt in dataloader:\n",
        "    optimizer.zero_grad()\n",
        "    output = model(Xt)\n",
        "\n",
        "    loss = criterion(torch.squeeze(output), torch.squeeze(yt))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_val += loss.item()\n",
        "    predicted = torch.squeeze(output).round().detach().numpy()\n",
        "    grtruth = torch.squeeze(yt).detach().numpy()\n",
        "    correct += np.sum((predicted == grtruth))\n",
        "    \n",
        "  return loss_val, correct"
      ],
      "metadata": {
        "id": "aoin-3qBaHKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_epoch(model, dataloader, criterion):\n",
        "    \n",
        "  model.eval()\n",
        "  loss_val, correct = 0.0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for Xt, yt in dataloader:\n",
        "      output = model(Xt)\n",
        "      loss = criterion(torch.squeeze(output), torch.squeeze(yt))\n",
        "\n",
        "      loss_val += loss.item()\n",
        "      predicted = torch.squeeze(output).round().detach().numpy()\n",
        "      grtruth = torch.squeeze(yt).detach().numpy()\n",
        "      correct += np.sum((predicted == grtruth))\n",
        "\n",
        "  return loss_val, correct"
      ],
      "metadata": {
        "id": "CkQCV32qauly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_cross_validation(model, X, y, n_splits, n_epochs, learning_rate):\n",
        "\n",
        "  Xt = torch.tensor(np.array(X), dtype=torch.float32)\n",
        "  yt = torch.tensor(np.array(y), dtype=torch.float32)\n",
        "  \n",
        "  splits = KFold(n_splits = n_splits, shuffle = True, random_state = 42)\n",
        "  \n",
        "  history = {'train_loss': [], 'test_loss': [],'train_acc':[],'test_acc':[]}\n",
        "\n",
        "  for fold, (train_idx, val_idx) in enumerate(splits.split(np.arange(len(yt)))):\n",
        "\n",
        "    print(f\"\\nFold {fold+1}\")\n",
        "\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    test_sampler = SubsetRandomSampler(val_idx)\n",
        "    train_loader = DataLoader(list(zip(Xt,yt)), sampler=train_sampler)\n",
        "    test_loader = DataLoader(list(zip(Xt,yt)), sampler=test_sampler)\n",
        "    \n",
        "    fold_model = model\n",
        "    criterion = torch.nn.BCELoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "      train_loss, train_correct = train_epoch(fold_model, train_loader, criterion, optimizer)\n",
        "      test_loss, test_correct = valid_epoch(fold_model, test_loader, criterion)\n",
        "\n",
        "      train_loss = round(train_loss / len(train_loader.sampler), 4)\n",
        "      train_acc = round(train_correct / len(train_loader.sampler) * 100, 4)\n",
        "      test_loss = round(test_loss / len(test_loader.sampler), 4)\n",
        "      test_acc = round(test_correct / len(test_loader.sampler) * 100, 4)\n",
        "\n",
        "      print(f\"Epoch:{epoch+1}/{n_epochs} AVG Training Loss:{train_loss} AVG Test Loss:{test_loss} AVG Training Acc {train_acc}% AVG Test Acc {test_acc}%\")\n",
        "\n",
        "    # scores for last epoch\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['test_loss'].append(test_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['test_acc'].append(test_acc)\n",
        "      \n",
        "  # mean scores for all folds\n",
        "  avg_train_loss = round(np.mean(history['train_loss']), 4)\n",
        "  avg_test_loss = round(np.mean(history['test_loss']), 4)\n",
        "  avg_train_acc = round(np.mean(history['train_acc']), 4)\n",
        "  avg_test_acc = round(np.mean(history['test_acc']), 4)\n",
        "\n",
        "  print(f\"\\nPerformance of {n_splits} fold cross validation\")\n",
        "  print(f\"Average Training Loss: {avg_train_loss} \\t Average Test Loss: {avg_test_loss} \\t Average Training Acc: {avg_train_acc} \\t Average Test Acc: {avg_test_acc}\")\n",
        "\n",
        "  return avg_test_acc"
      ],
      "metadata": {
        "id": "avLbUcLQcHu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One hot encoding"
      ],
      "metadata": {
        "id": "YTJOJHu1HLQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = get_subsets(df_ohe)\n",
        "input_dim = X.shape[1]\n",
        "output_dim = 1\n",
        "model = LogisticRegression(input_dim, output_dim)"
      ],
      "metadata": {
        "id": "ZLOsB0GncXXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_ohe = model_cross_validation(model, X, y, n_splits=10, n_epochs=10, learning_rate=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgOctCoNIoDF",
        "outputId": "00f47924-0bd4-4608-b19a-457952d3310b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1\n",
            "Epoch:1/10 AVG Training Loss:0.4972 AVG Test Loss:0.4907 AVG Training Acc 76.3262% AVG Test Acc 76.2069%\n",
            "Epoch:2/10 AVG Training Loss:0.4585 AVG Test Loss:0.4835 AVG Training Acc 78.8189% AVG Test Acc 77.0115%\n",
            "Epoch:3/10 AVG Training Loss:0.452 AVG Test Loss:0.4887 AVG Training Acc 78.8572% AVG Test Acc 76.3218%\n",
            "Epoch:4/10 AVG Training Loss:0.4492 AVG Test Loss:0.4831 AVG Training Acc 79.1768% AVG Test Acc 76.5517%\n",
            "Epoch:5/10 AVG Training Loss:0.4473 AVG Test Loss:0.4855 AVG Training Acc 78.985% AVG Test Acc 76.8966%\n",
            "Epoch:6/10 AVG Training Loss:0.4473 AVG Test Loss:0.4811 AVG Training Acc 78.8956% AVG Test Acc 76.6667%\n",
            "Epoch:7/10 AVG Training Loss:0.4462 AVG Test Loss:0.484 AVG Training Acc 78.9339% AVG Test Acc 76.5517%\n",
            "Epoch:8/10 AVG Training Loss:0.4468 AVG Test Loss:0.4831 AVG Training Acc 78.8317% AVG Test Acc 76.5517%\n",
            "Epoch:9/10 AVG Training Loss:0.447 AVG Test Loss:0.4813 AVG Training Acc 79.0106% AVG Test Acc 76.7816%\n",
            "Epoch:10/10 AVG Training Loss:0.4467 AVG Test Loss:0.4822 AVG Training Acc 79.0106% AVG Test Acc 76.8966%\n",
            "\n",
            "Fold 2\n",
            "Epoch:1/10 AVG Training Loss:0.4518 AVG Test Loss:0.4381 AVG Training Acc 78.9723% AVG Test Acc 78.046%\n",
            "Epoch:2/10 AVG Training Loss:0.4515 AVG Test Loss:0.4393 AVG Training Acc 78.9723% AVG Test Acc 78.3908%\n",
            "Epoch:3/10 AVG Training Loss:0.4506 AVG Test Loss:0.441 AVG Training Acc 78.9339% AVG Test Acc 77.5862%\n",
            "Epoch:4/10 AVG Training Loss:0.4505 AVG Test Loss:0.4457 AVG Training Acc 78.8444% AVG Test Acc 77.2414%\n",
            "Epoch:5/10 AVG Training Loss:0.4514 AVG Test Loss:0.4436 AVG Training Acc 78.9467% AVG Test Acc 77.4713%\n",
            "Epoch:6/10 AVG Training Loss:0.4506 AVG Test Loss:0.443 AVG Training Acc 78.5249% AVG Test Acc 77.5862%\n",
            "Epoch:7/10 AVG Training Loss:0.4506 AVG Test Loss:0.4501 AVG Training Acc 79.049% AVG Test Acc 76.8966%\n",
            "Epoch:8/10 AVG Training Loss:0.4512 AVG Test Loss:0.4493 AVG Training Acc 78.8444% AVG Test Acc 77.7011%\n",
            "Epoch:9/10 AVG Training Loss:0.4511 AVG Test Loss:0.4467 AVG Training Acc 78.7933% AVG Test Acc 76.8966%\n",
            "Epoch:10/10 AVG Training Loss:0.4504 AVG Test Loss:0.4515 AVG Training Acc 78.9467% AVG Test Acc 77.4713%\n",
            "\n",
            "Fold 3\n",
            "Epoch:1/10 AVG Training Loss:0.4499 AVG Test Loss:0.4532 AVG Training Acc 78.7677% AVG Test Acc 78.9655%\n",
            "Epoch:2/10 AVG Training Loss:0.4493 AVG Test Loss:0.4555 AVG Training Acc 78.7422% AVG Test Acc 78.6207%\n",
            "Epoch:3/10 AVG Training Loss:0.448 AVG Test Loss:0.4632 AVG Training Acc 78.7805% AVG Test Acc 77.8161%\n",
            "Epoch:4/10 AVG Training Loss:0.4494 AVG Test Loss:0.4582 AVG Training Acc 78.8828% AVG Test Acc 77.4713%\n",
            "Epoch:5/10 AVG Training Loss:0.4492 AVG Test Loss:0.4587 AVG Training Acc 78.755% AVG Test Acc 77.7011%\n",
            "Epoch:6/10 AVG Training Loss:0.4494 AVG Test Loss:0.4582 AVG Training Acc 78.7933% AVG Test Acc 78.046%\n",
            "Epoch:7/10 AVG Training Loss:0.45 AVG Test Loss:0.4566 AVG Training Acc 78.7422% AVG Test Acc 78.1609%\n",
            "Epoch:8/10 AVG Training Loss:0.449 AVG Test Loss:0.4608 AVG Training Acc 78.6016% AVG Test Acc 77.931%\n",
            "Epoch:9/10 AVG Training Loss:0.4491 AVG Test Loss:0.4647 AVG Training Acc 78.7933% AVG Test Acc 77.4713%\n",
            "Epoch:10/10 AVG Training Loss:0.4504 AVG Test Loss:0.4573 AVG Training Acc 78.6143% AVG Test Acc 77.931%\n",
            "\n",
            "Fold 4\n",
            "Epoch:1/10 AVG Training Loss:0.4488 AVG Test Loss:0.4596 AVG Training Acc 78.9749% AVG Test Acc 78.7112%\n",
            "Epoch:2/10 AVG Training Loss:0.4486 AVG Test Loss:0.458 AVG Training Acc 78.796% AVG Test Acc 78.3659%\n",
            "Epoch:3/10 AVG Training Loss:0.4488 AVG Test Loss:0.4735 AVG Training Acc 79.0772% AVG Test Acc 77.1001%\n",
            "Epoch:4/10 AVG Training Loss:0.4482 AVG Test Loss:0.4643 AVG Training Acc 78.9494% AVG Test Acc 77.6755%\n",
            "Epoch:5/10 AVG Training Loss:0.4481 AVG Test Loss:0.4632 AVG Training Acc 78.8344% AVG Test Acc 78.7112%\n",
            "Epoch:6/10 AVG Training Loss:0.4491 AVG Test Loss:0.4642 AVG Training Acc 78.8983% AVG Test Acc 77.6755%\n",
            "Epoch:7/10 AVG Training Loss:0.4484 AVG Test Loss:0.4596 AVG Training Acc 78.911% AVG Test Acc 77.7906%\n",
            "Epoch:8/10 AVG Training Loss:0.449 AVG Test Loss:0.4591 AVG Training Acc 78.8983% AVG Test Acc 78.0207%\n",
            "Epoch:9/10 AVG Training Loss:0.4491 AVG Test Loss:0.4599 AVG Training Acc 79.0005% AVG Test Acc 78.1358%\n",
            "Epoch:10/10 AVG Training Loss:0.4488 AVG Test Loss:0.4711 AVG Training Acc 78.8216% AVG Test Acc 76.87%\n",
            "\n",
            "Fold 5\n",
            "Epoch:1/10 AVG Training Loss:0.4498 AVG Test Loss:0.4514 AVG Training Acc 78.7321% AVG Test Acc 78.481%\n",
            "Epoch:2/10 AVG Training Loss:0.449 AVG Test Loss:0.4576 AVG Training Acc 78.9366% AVG Test Acc 77.4453%\n",
            "Epoch:3/10 AVG Training Loss:0.4488 AVG Test Loss:0.4649 AVG Training Acc 79.1667% AVG Test Acc 78.8262%\n",
            "Epoch:4/10 AVG Training Loss:0.449 AVG Test Loss:0.4564 AVG Training Acc 78.8344% AVG Test Acc 77.5604%\n",
            "Epoch:5/10 AVG Training Loss:0.4497 AVG Test Loss:0.4575 AVG Training Acc 78.7193% AVG Test Acc 77.2152%\n",
            "Epoch:6/10 AVG Training Loss:0.448 AVG Test Loss:0.4605 AVG Training Acc 79.0516% AVG Test Acc 78.0207%\n",
            "Epoch:7/10 AVG Training Loss:0.4495 AVG Test Loss:0.4582 AVG Training Acc 78.9494% AVG Test Acc 77.6755%\n",
            "Epoch:8/10 AVG Training Loss:0.4487 AVG Test Loss:0.4573 AVG Training Acc 79.1539% AVG Test Acc 77.4453%\n",
            "Epoch:9/10 AVG Training Loss:0.4491 AVG Test Loss:0.4585 AVG Training Acc 78.681% AVG Test Acc 78.5961%\n",
            "Epoch:10/10 AVG Training Loss:0.449 AVG Test Loss:0.472 AVG Training Acc 78.566% AVG Test Acc 78.5961%\n",
            "\n",
            "Fold 6\n",
            "Epoch:1/10 AVG Training Loss:0.4507 AVG Test Loss:0.4459 AVG Training Acc 78.4637% AVG Test Acc 79.6318%\n",
            "Epoch:2/10 AVG Training Loss:0.4491 AVG Test Loss:0.4451 AVG Training Acc 79.0133% AVG Test Acc 80.2071%\n",
            "Epoch:3/10 AVG Training Loss:0.45 AVG Test Loss:0.4508 AVG Training Acc 78.9238% AVG Test Acc 80.0921%\n",
            "Epoch:4/10 AVG Training Loss:0.4496 AVG Test Loss:0.4507 AVG Training Acc 78.6682% AVG Test Acc 79.8619%\n",
            "Epoch:5/10 AVG Training Loss:0.4492 AVG Test Loss:0.4532 AVG Training Acc 78.9622% AVG Test Acc 80.5524%\n",
            "Epoch:6/10 AVG Training Loss:0.4501 AVG Test Loss:0.4486 AVG Training Acc 78.6299% AVG Test Acc 80.0921%\n",
            "Epoch:7/10 AVG Training Loss:0.4499 AVG Test Loss:0.4492 AVG Training Acc 78.6426% AVG Test Acc 80.4373%\n",
            "Epoch:8/10 AVG Training Loss:0.4496 AVG Test Loss:0.4499 AVG Training Acc 78.7832% AVG Test Acc 79.7468%\n",
            "Epoch:9/10 AVG Training Loss:0.4499 AVG Test Loss:0.4489 AVG Training Acc 78.7065% AVG Test Acc 80.4373%\n",
            "Epoch:10/10 AVG Training Loss:0.4505 AVG Test Loss:0.4484 AVG Training Acc 78.566% AVG Test Acc 80.7825%\n",
            "\n",
            "Fold 7\n",
            "Epoch:1/10 AVG Training Loss:0.45 AVG Test Loss:0.446 AVG Training Acc 79.1155% AVG Test Acc 78.3659%\n",
            "Epoch:2/10 AVG Training Loss:0.45 AVG Test Loss:0.4616 AVG Training Acc 78.7704% AVG Test Acc 78.8262%\n",
            "Epoch:3/10 AVG Training Loss:0.4503 AVG Test Loss:0.4496 AVG Training Acc 78.911% AVG Test Acc 78.5961%\n",
            "Epoch:4/10 AVG Training Loss:0.4496 AVG Test Loss:0.4521 AVG Training Acc 78.8983% AVG Test Acc 78.3659%\n",
            "Epoch:5/10 AVG Training Loss:0.4505 AVG Test Loss:0.4526 AVG Training Acc 78.5532% AVG Test Acc 78.481%\n",
            "Epoch:6/10 AVG Training Loss:0.4492 AVG Test Loss:0.4518 AVG Training Acc 78.8855% AVG Test Acc 78.2509%\n",
            "Epoch:7/10 AVG Training Loss:0.4499 AVG Test Loss:0.4528 AVG Training Acc 78.8727% AVG Test Acc 78.1358%\n",
            "Epoch:8/10 AVG Training Loss:0.4495 AVG Test Loss:0.4573 AVG Training Acc 78.9494% AVG Test Acc 78.7112%\n",
            "Epoch:9/10 AVG Training Loss:0.45 AVG Test Loss:0.4576 AVG Training Acc 79.1922% AVG Test Acc 77.3303%\n",
            "Epoch:10/10 AVG Training Loss:0.4498 AVG Test Loss:0.4503 AVG Training Acc 79.205% AVG Test Acc 78.5961%\n",
            "\n",
            "Fold 8\n",
            "Epoch:1/10 AVG Training Loss:0.4512 AVG Test Loss:0.4335 AVG Training Acc 78.4765% AVG Test Acc 80.6674%\n",
            "Epoch:2/10 AVG Training Loss:0.4519 AVG Test Loss:0.439 AVG Training Acc 78.4254% AVG Test Acc 80.4373%\n",
            "Epoch:3/10 AVG Training Loss:0.452 AVG Test Loss:0.431 AVG Training Acc 78.8855% AVG Test Acc 80.7825%\n",
            "Epoch:4/10 AVG Training Loss:0.4515 AVG Test Loss:0.4323 AVG Training Acc 78.6171% AVG Test Acc 80.6674%\n",
            "Epoch:5/10 AVG Training Loss:0.4519 AVG Test Loss:0.4303 AVG Training Acc 78.566% AVG Test Acc 80.3222%\n",
            "Epoch:6/10 AVG Training Loss:0.4503 AVG Test Loss:0.4412 AVG Training Acc 78.7704% AVG Test Acc 79.977%\n",
            "Epoch:7/10 AVG Training Loss:0.4516 AVG Test Loss:0.4361 AVG Training Acc 78.5787% AVG Test Acc 81.0127%\n",
            "Epoch:8/10 AVG Training Loss:0.4519 AVG Test Loss:0.4316 AVG Training Acc 78.7321% AVG Test Acc 80.8976%\n",
            "Epoch:9/10 AVG Training Loss:0.4517 AVG Test Loss:0.4315 AVG Training Acc 78.5148% AVG Test Acc 80.6674%\n",
            "Epoch:10/10 AVG Training Loss:0.4502 AVG Test Loss:0.4309 AVG Training Acc 78.8983% AVG Test Acc 80.7825%\n",
            "\n",
            "Fold 9\n",
            "Epoch:1/10 AVG Training Loss:0.448 AVG Test Loss:0.4612 AVG Training Acc 78.681% AVG Test Acc 79.0564%\n",
            "Epoch:2/10 AVG Training Loss:0.4475 AVG Test Loss:0.4645 AVG Training Acc 78.8983% AVG Test Acc 78.9413%\n",
            "Epoch:3/10 AVG Training Loss:0.4476 AVG Test Loss:0.4714 AVG Training Acc 79.0133% AVG Test Acc 78.1358%\n",
            "Epoch:4/10 AVG Training Loss:0.4484 AVG Test Loss:0.4794 AVG Training Acc 79.0133% AVG Test Acc 77.4453%\n",
            "Epoch:5/10 AVG Training Loss:0.4472 AVG Test Loss:0.468 AVG Training Acc 78.9622% AVG Test Acc 78.481%\n",
            "Epoch:6/10 AVG Training Loss:0.4474 AVG Test Loss:0.4658 AVG Training Acc 79.1922% AVG Test Acc 78.7112%\n",
            "Epoch:7/10 AVG Training Loss:0.4482 AVG Test Loss:0.4701 AVG Training Acc 78.7577% AVG Test Acc 78.481%\n",
            "Epoch:8/10 AVG Training Loss:0.4476 AVG Test Loss:0.469 AVG Training Acc 78.681% AVG Test Acc 78.0207%\n",
            "Epoch:9/10 AVG Training Loss:0.4477 AVG Test Loss:0.4673 AVG Training Acc 78.5532% AVG Test Acc 79.5167%\n",
            "Epoch:10/10 AVG Training Loss:0.4471 AVG Test Loss:0.4768 AVG Training Acc 79.09% AVG Test Acc 79.2865%\n",
            "\n",
            "Fold 10\n",
            "Epoch:1/10 AVG Training Loss:0.4529 AVG Test Loss:0.4347 AVG Training Acc 78.6938% AVG Test Acc 79.4016%\n",
            "Epoch:2/10 AVG Training Loss:0.4522 AVG Test Loss:0.4311 AVG Training Acc 78.8344% AVG Test Acc 78.3659%\n",
            "Epoch:3/10 AVG Training Loss:0.4526 AVG Test Loss:0.4287 AVG Training Acc 78.6682% AVG Test Acc 80.2071%\n",
            "Epoch:4/10 AVG Training Loss:0.4525 AVG Test Loss:0.4247 AVG Training Acc 78.566% AVG Test Acc 80.0921%\n",
            "Epoch:5/10 AVG Training Loss:0.4519 AVG Test Loss:0.4222 AVG Training Acc 78.4509% AVG Test Acc 81.0127%\n",
            "Epoch:6/10 AVG Training Loss:0.4533 AVG Test Loss:0.4258 AVG Training Acc 78.6682% AVG Test Acc 80.4373%\n",
            "Epoch:7/10 AVG Training Loss:0.4528 AVG Test Loss:0.425 AVG Training Acc 78.8344% AVG Test Acc 81.3579%\n",
            "Epoch:8/10 AVG Training Loss:0.4517 AVG Test Loss:0.4251 AVG Training Acc 78.681% AVG Test Acc 80.8976%\n",
            "Epoch:9/10 AVG Training Loss:0.4525 AVG Test Loss:0.4266 AVG Training Acc 78.7065% AVG Test Acc 80.2071%\n",
            "Epoch:10/10 AVG Training Loss:0.4523 AVG Test Loss:0.4346 AVG Training Acc 78.4509% AVG Test Acc 80.2071%\n",
            "\n",
            "Performance of 10 fold cross validation\n",
            "Average Training Loss: 0.4495 \t Average Test Loss: 0.4575 \t Average Training Acc: 78.8169 \t Average Test Acc: 78.742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Target encoding"
      ],
      "metadata": {
        "id": "MWHtgYcZbJ7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = get_subsets(df_te)\n",
        "input_dim = X.shape[1]\n",
        "output_dim = 1\n",
        "model = LogisticRegression(input_dim, output_dim)"
      ],
      "metadata": {
        "id": "7XTwz8faSoZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_te = model_cross_validation(model, X, y, n_splits=10, n_epochs=10, learning_rate=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjmJZxanS9RY",
        "outputId": "5fe504d7-10c6-4827-932f-5f16ccfc13eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1\n",
            "Epoch:1/10 AVG Training Loss:0.6233 AVG Test Loss:0.5902 AVG Training Acc 68.4264% AVG Test Acc 70.8046%\n",
            "Epoch:2/10 AVG Training Loss:0.5625 AVG Test Loss:0.5622 AVG Training Acc 73.9742% AVG Test Acc 72.6437%\n",
            "Epoch:3/10 AVG Training Loss:0.5492 AVG Test Loss:0.5579 AVG Training Acc 73.6418% AVG Test Acc 75.0575%\n",
            "Epoch:4/10 AVG Training Loss:0.5431 AVG Test Loss:0.5505 AVG Training Acc 74.2682% AVG Test Acc 72.2989%\n",
            "Epoch:5/10 AVG Training Loss:0.5379 AVG Test Loss:0.5453 AVG Training Acc 74.4088% AVG Test Acc 73.908%\n",
            "Epoch:6/10 AVG Training Loss:0.5339 AVG Test Loss:0.5479 AVG Training Acc 74.8178% AVG Test Acc 75.1724%\n",
            "Epoch:7/10 AVG Training Loss:0.5306 AVG Test Loss:0.5395 AVG Training Acc 74.7795% AVG Test Acc 73.7931%\n",
            "Epoch:8/10 AVG Training Loss:0.5279 AVG Test Loss:0.5435 AVG Training Acc 74.6261% AVG Test Acc 75.1724%\n",
            "Epoch:9/10 AVG Training Loss:0.5258 AVG Test Loss:0.5414 AVG Training Acc 75.1246% AVG Test Acc 75.1724%\n",
            "Epoch:10/10 AVG Training Loss:0.5239 AVG Test Loss:0.5344 AVG Training Acc 75.163% AVG Test Acc 75.2874%\n",
            "\n",
            "Fold 2\n",
            "Epoch:1/10 AVG Training Loss:0.5252 AVG Test Loss:0.5096 AVG Training Acc 75.163% AVG Test Acc 76.2069%\n",
            "Epoch:2/10 AVG Training Loss:0.5232 AVG Test Loss:0.5066 AVG Training Acc 75.1758% AVG Test Acc 76.092%\n",
            "Epoch:3/10 AVG Training Loss:0.5217 AVG Test Loss:0.5054 AVG Training Acc 75.278% AVG Test Acc 76.2069%\n",
            "Epoch:4/10 AVG Training Loss:0.5208 AVG Test Loss:0.5065 AVG Training Acc 75.4698% AVG Test Acc 75.5172%\n",
            "Epoch:5/10 AVG Training Loss:0.5198 AVG Test Loss:0.5035 AVG Training Acc 75.4314% AVG Test Acc 76.5517%\n",
            "Epoch:6/10 AVG Training Loss:0.5189 AVG Test Loss:0.5034 AVG Training Acc 75.4953% AVG Test Acc 76.5517%\n",
            "Epoch:7/10 AVG Training Loss:0.5179 AVG Test Loss:0.5018 AVG Training Acc 75.5209% AVG Test Acc 76.6667%\n",
            "Epoch:8/10 AVG Training Loss:0.5169 AVG Test Loss:0.5009 AVG Training Acc 75.5337% AVG Test Acc 76.5517%\n",
            "Epoch:9/10 AVG Training Loss:0.5166 AVG Test Loss:0.5003 AVG Training Acc 75.3164% AVG Test Acc 76.6667%\n",
            "Epoch:10/10 AVG Training Loss:0.5161 AVG Test Loss:0.5018 AVG Training Acc 75.8277% AVG Test Acc 75.977%\n",
            "\n",
            "Fold 3\n",
            "Epoch:1/10 AVG Training Loss:0.5129 AVG Test Loss:0.5171 AVG Training Acc 75.3547% AVG Test Acc 76.092%\n",
            "Epoch:2/10 AVG Training Loss:0.5127 AVG Test Loss:0.5156 AVG Training Acc 75.6871% AVG Test Acc 75.7471%\n",
            "Epoch:3/10 AVG Training Loss:0.5122 AVG Test Loss:0.5178 AVG Training Acc 75.8277% AVG Test Acc 76.5517%\n",
            "Epoch:4/10 AVG Training Loss:0.5119 AVG Test Loss:0.5143 AVG Training Acc 75.8277% AVG Test Acc 76.092%\n",
            "Epoch:5/10 AVG Training Loss:0.5117 AVG Test Loss:0.5144 AVG Training Acc 75.6359% AVG Test Acc 75.977%\n",
            "Epoch:6/10 AVG Training Loss:0.5112 AVG Test Loss:0.5133 AVG Training Acc 75.9939% AVG Test Acc 76.2069%\n",
            "Epoch:7/10 AVG Training Loss:0.5107 AVG Test Loss:0.5145 AVG Training Acc 75.8277% AVG Test Acc 76.092%\n",
            "Epoch:8/10 AVG Training Loss:0.5103 AVG Test Loss:0.5127 AVG Training Acc 75.7638% AVG Test Acc 75.8621%\n",
            "Epoch:9/10 AVG Training Loss:0.5098 AVG Test Loss:0.5126 AVG Training Acc 76.045% AVG Test Acc 75.977%\n",
            "Epoch:10/10 AVG Training Loss:0.5095 AVG Test Loss:0.5143 AVG Training Acc 75.8916% AVG Test Acc 76.2069%\n",
            "\n",
            "Fold 4\n",
            "Epoch:1/10 AVG Training Loss:0.5085 AVG Test Loss:0.523 AVG Training Acc 76.2142% AVG Test Acc 73.7629%\n",
            "Epoch:2/10 AVG Training Loss:0.5084 AVG Test Loss:0.5198 AVG Training Acc 76.0608% AVG Test Acc 74.5685%\n",
            "Epoch:3/10 AVG Training Loss:0.5083 AVG Test Loss:0.5201 AVG Training Acc 76.2014% AVG Test Acc 74.7986%\n",
            "Epoch:4/10 AVG Training Loss:0.5073 AVG Test Loss:0.5219 AVG Training Acc 76.0481% AVG Test Acc 75.374%\n",
            "Epoch:5/10 AVG Training Loss:0.5076 AVG Test Loss:0.5199 AVG Training Acc 76.3037% AVG Test Acc 74.3383%\n",
            "Epoch:6/10 AVG Training Loss:0.5071 AVG Test Loss:0.5222 AVG Training Acc 76.0353% AVG Test Acc 75.2589%\n",
            "Epoch:7/10 AVG Training Loss:0.5073 AVG Test Loss:0.5194 AVG Training Acc 76.112% AVG Test Acc 74.4534%\n",
            "Epoch:8/10 AVG Training Loss:0.5058 AVG Test Loss:0.5305 AVG Training Acc 76.2526% AVG Test Acc 73.4177%\n",
            "Epoch:9/10 AVG Training Loss:0.5068 AVG Test Loss:0.5189 AVG Training Acc 76.0225% AVG Test Acc 74.7986%\n",
            "Epoch:10/10 AVG Training Loss:0.506 AVG Test Loss:0.5184 AVG Training Acc 76.2398% AVG Test Acc 74.9137%\n",
            "\n",
            "Fold 5\n",
            "Epoch:1/10 AVG Training Loss:0.5057 AVG Test Loss:0.5227 AVG Training Acc 76.1631% AVG Test Acc 73.7629%\n",
            "Epoch:2/10 AVG Training Loss:0.5053 AVG Test Loss:0.5351 AVG Training Acc 76.3037% AVG Test Acc 73.1876%\n",
            "Epoch:3/10 AVG Training Loss:0.5053 AVG Test Loss:0.5228 AVG Training Acc 76.3676% AVG Test Acc 73.5328%\n",
            "Epoch:4/10 AVG Training Loss:0.5047 AVG Test Loss:0.5231 AVG Training Acc 76.4187% AVG Test Acc 73.4177%\n",
            "Epoch:5/10 AVG Training Loss:0.5049 AVG Test Loss:0.5234 AVG Training Acc 76.1631% AVG Test Acc 73.5328%\n",
            "Epoch:6/10 AVG Training Loss:0.5049 AVG Test Loss:0.5248 AVG Training Acc 76.342% AVG Test Acc 74.6835%\n",
            "Epoch:7/10 AVG Training Loss:0.5044 AVG Test Loss:0.5259 AVG Training Acc 76.4571% AVG Test Acc 73.4177%\n",
            "Epoch:8/10 AVG Training Loss:0.5047 AVG Test Loss:0.5265 AVG Training Acc 76.3931% AVG Test Acc 74.4534%\n",
            "Epoch:9/10 AVG Training Loss:0.5042 AVG Test Loss:0.5233 AVG Training Acc 76.4954% AVG Test Acc 74.5685%\n",
            "Epoch:10/10 AVG Training Loss:0.5047 AVG Test Loss:0.5288 AVG Training Acc 76.5082% AVG Test Acc 73.878%\n",
            "\n",
            "Fold 6\n",
            "Epoch:1/10 AVG Training Loss:0.5076 AVG Test Loss:0.4941 AVG Training Acc 75.9842% AVG Test Acc 77.6755%\n",
            "Epoch:2/10 AVG Training Loss:0.5072 AVG Test Loss:0.4969 AVG Training Acc 75.8819% AVG Test Acc 78.3659%\n",
            "Epoch:3/10 AVG Training Loss:0.5069 AVG Test Loss:0.4951 AVG Training Acc 76.1247% AVG Test Acc 78.2509%\n",
            "Epoch:4/10 AVG Training Loss:0.507 AVG Test Loss:0.4949 AVG Training Acc 75.8947% AVG Test Acc 78.0207%\n",
            "Epoch:5/10 AVG Training Loss:0.5067 AVG Test Loss:0.4936 AVG Training Acc 76.0864% AVG Test Acc 78.0207%\n",
            "Epoch:6/10 AVG Training Loss:0.5063 AVG Test Loss:0.494 AVG Training Acc 75.9842% AVG Test Acc 77.6755%\n",
            "Epoch:7/10 AVG Training Loss:0.5067 AVG Test Loss:0.4974 AVG Training Acc 75.7157% AVG Test Acc 77.9056%\n",
            "Epoch:8/10 AVG Training Loss:0.5063 AVG Test Loss:0.4975 AVG Training Acc 76.0736% AVG Test Acc 77.9056%\n",
            "Epoch:9/10 AVG Training Loss:0.5066 AVG Test Loss:0.4937 AVG Training Acc 76.0097% AVG Test Acc 77.6755%\n",
            "Epoch:10/10 AVG Training Loss:0.5059 AVG Test Loss:0.4968 AVG Training Acc 76.0225% AVG Test Acc 77.9056%\n",
            "\n",
            "Fold 7\n",
            "Epoch:1/10 AVG Training Loss:0.5045 AVG Test Loss:0.512 AVG Training Acc 76.1887% AVG Test Acc 75.9494%\n",
            "Epoch:2/10 AVG Training Loss:0.5042 AVG Test Loss:0.5167 AVG Training Acc 76.0736% AVG Test Acc 75.9494%\n",
            "Epoch:3/10 AVG Training Loss:0.5046 AVG Test Loss:0.5174 AVG Training Acc 76.3037% AVG Test Acc 74.4534%\n",
            "Epoch:4/10 AVG Training Loss:0.5043 AVG Test Loss:0.5126 AVG Training Acc 76.4187% AVG Test Acc 74.9137%\n",
            "Epoch:5/10 AVG Training Loss:0.5031 AVG Test Loss:0.5119 AVG Training Acc 76.1759% AVG Test Acc 75.8343%\n",
            "Epoch:6/10 AVG Training Loss:0.5033 AVG Test Loss:0.5123 AVG Training Acc 76.3804% AVG Test Acc 75.9494%\n",
            "Epoch:7/10 AVG Training Loss:0.5039 AVG Test Loss:0.5132 AVG Training Acc 76.4954% AVG Test Acc 76.5247%\n",
            "Epoch:8/10 AVG Training Loss:0.5035 AVG Test Loss:0.5119 AVG Training Acc 76.1887% AVG Test Acc 76.1795%\n",
            "Epoch:9/10 AVG Training Loss:0.5034 AVG Test Loss:0.5172 AVG Training Acc 76.521% AVG Test Acc 75.6041%\n",
            "Epoch:10/10 AVG Training Loss:0.5038 AVG Test Loss:0.5127 AVG Training Acc 76.4059% AVG Test Acc 75.9494%\n",
            "\n",
            "Fold 8\n",
            "Epoch:1/10 AVG Training Loss:0.5063 AVG Test Loss:0.4967 AVG Training Acc 76.1247% AVG Test Acc 76.6398%\n",
            "Epoch:2/10 AVG Training Loss:0.5064 AVG Test Loss:0.4865 AVG Training Acc 76.0736% AVG Test Acc 78.1358%\n",
            "Epoch:3/10 AVG Training Loss:0.5061 AVG Test Loss:0.4874 AVG Training Acc 75.9075% AVG Test Acc 77.5604%\n",
            "Epoch:4/10 AVG Training Loss:0.5061 AVG Test Loss:0.4906 AVG Training Acc 76.1631% AVG Test Acc 77.2152%\n",
            "Epoch:5/10 AVG Training Loss:0.5065 AVG Test Loss:0.4884 AVG Training Acc 76.3931% AVG Test Acc 77.5604%\n",
            "Epoch:6/10 AVG Training Loss:0.5059 AVG Test Loss:0.4997 AVG Training Acc 76.1759% AVG Test Acc 76.6398%\n",
            "Epoch:7/10 AVG Training Loss:0.5062 AVG Test Loss:0.49 AVG Training Acc 76.1503% AVG Test Acc 77.2152%\n",
            "Epoch:8/10 AVG Training Loss:0.5059 AVG Test Loss:0.4867 AVG Training Acc 76.2526% AVG Test Acc 78.0207%\n",
            "Epoch:9/10 AVG Training Loss:0.505 AVG Test Loss:0.4866 AVG Training Acc 76.2781% AVG Test Acc 78.0207%\n",
            "Epoch:10/10 AVG Training Loss:0.5061 AVG Test Loss:0.4874 AVG Training Acc 75.9714% AVG Test Acc 77.6755%\n",
            "\n",
            "Fold 9\n",
            "Epoch:1/10 AVG Training Loss:0.5039 AVG Test Loss:0.5106 AVG Training Acc 76.227% AVG Test Acc 75.9494%\n",
            "Epoch:2/10 AVG Training Loss:0.5035 AVG Test Loss:0.512 AVG Training Acc 76.4571% AVG Test Acc 75.7192%\n",
            "Epoch:3/10 AVG Training Loss:0.5036 AVG Test Loss:0.5104 AVG Training Acc 76.3165% AVG Test Acc 75.7192%\n",
            "Epoch:4/10 AVG Training Loss:0.5029 AVG Test Loss:0.5156 AVG Training Acc 76.1759% AVG Test Acc 75.9494%\n",
            "Epoch:5/10 AVG Training Loss:0.503 AVG Test Loss:0.5104 AVG Training Acc 76.2014% AVG Test Acc 76.2946%\n",
            "Epoch:6/10 AVG Training Loss:0.503 AVG Test Loss:0.5133 AVG Training Acc 76.3165% AVG Test Acc 75.4891%\n",
            "Epoch:7/10 AVG Training Loss:0.5034 AVG Test Loss:0.5108 AVG Training Acc 76.2142% AVG Test Acc 75.9494%\n",
            "Epoch:8/10 AVG Training Loss:0.5034 AVG Test Loss:0.5123 AVG Training Acc 76.3931% AVG Test Acc 75.8343%\n",
            "Epoch:9/10 AVG Training Loss:0.5032 AVG Test Loss:0.51 AVG Training Acc 76.1887% AVG Test Acc 76.4097%\n",
            "Epoch:10/10 AVG Training Loss:0.5031 AVG Test Loss:0.5114 AVG Training Acc 76.3037% AVG Test Acc 75.6041%\n",
            "\n",
            "Fold 10\n",
            "Epoch:1/10 AVG Training Loss:0.5067 AVG Test Loss:0.4884 AVG Training Acc 75.8947% AVG Test Acc 78.481%\n",
            "Epoch:2/10 AVG Training Loss:0.5063 AVG Test Loss:0.4866 AVG Training Acc 76.1247% AVG Test Acc 78.2509%\n",
            "Epoch:3/10 AVG Training Loss:0.5063 AVG Test Loss:0.4863 AVG Training Acc 75.8563% AVG Test Acc 78.2509%\n",
            "Epoch:4/10 AVG Training Loss:0.5067 AVG Test Loss:0.481 AVG Training Acc 75.8563% AVG Test Acc 77.4453%\n",
            "Epoch:5/10 AVG Training Loss:0.5063 AVG Test Loss:0.4814 AVG Training Acc 75.9969% AVG Test Acc 78.1358%\n",
            "Epoch:6/10 AVG Training Loss:0.506 AVG Test Loss:0.4819 AVG Training Acc 76.112% AVG Test Acc 78.0207%\n",
            "Epoch:7/10 AVG Training Loss:0.5062 AVG Test Loss:0.4816 AVG Training Acc 75.8947% AVG Test Acc 78.1358%\n",
            "Epoch:8/10 AVG Training Loss:0.5059 AVG Test Loss:0.4866 AVG Training Acc 76.2909% AVG Test Acc 78.0207%\n",
            "Epoch:9/10 AVG Training Loss:0.5056 AVG Test Loss:0.4835 AVG Training Acc 76.0481% AVG Test Acc 78.1358%\n",
            "Epoch:10/10 AVG Training Loss:0.5054 AVG Test Loss:0.4817 AVG Training Acc 75.9586% AVG Test Acc 77.1001%\n",
            "\n",
            "Performance of 10 fold cross validation\n",
            "Average Training Loss: 0.5085 \t Average Test Loss: 0.5088 \t Average Training Acc: 76.0292 \t Average Test Acc: 76.0498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Leave one out encoding"
      ],
      "metadata": {
        "id": "Hqe_te-cbOA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = get_subsets(df_looe)\n",
        "input_dim = X.shape[1]\n",
        "output_dim = 1\n",
        "model = LogisticRegression(input_dim, output_dim)"
      ],
      "metadata": {
        "id": "aukVih24UpuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_looe = model_cross_validation(model, X, y, n_splits=10, n_epochs=10, learning_rate=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSS1i5E6Uu1v",
        "outputId": "4e13ca62-9876-42e2-f14e-a9a23e078d2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fold 1\n",
            "Epoch:1/10 AVG Training Loss:0.6162 AVG Test Loss:0.5772 AVG Training Acc 69.4746% AVG Test Acc 72.5287%\n",
            "Epoch:2/10 AVG Training Loss:0.5588 AVG Test Loss:0.5613 AVG Training Acc 73.9358% AVG Test Acc 74.8276%\n",
            "Epoch:3/10 AVG Training Loss:0.5469 AVG Test Loss:0.5536 AVG Training Acc 74.1404% AVG Test Acc 74.9425%\n",
            "Epoch:4/10 AVG Training Loss:0.5413 AVG Test Loss:0.5578 AVG Training Acc 74.2937% AVG Test Acc 75.1724%\n",
            "Epoch:5/10 AVG Training Loss:0.5365 AVG Test Loss:0.5579 AVG Training Acc 74.6517% AVG Test Acc 75.5172%\n",
            "Epoch:6/10 AVG Training Loss:0.534 AVG Test Loss:0.5497 AVG Training Acc 74.5622% AVG Test Acc 75.0575%\n",
            "Epoch:7/10 AVG Training Loss:0.5306 AVG Test Loss:0.5425 AVG Training Acc 74.7795% AVG Test Acc 75.2874%\n",
            "Epoch:8/10 AVG Training Loss:0.529 AVG Test Loss:0.5389 AVG Training Acc 74.984% AVG Test Acc 74.5977%\n",
            "Epoch:9/10 AVG Training Loss:0.527 AVG Test Loss:0.5365 AVG Training Acc 75.1758% AVG Test Acc 74.5977%\n",
            "Epoch:10/10 AVG Training Loss:0.5256 AVG Test Loss:0.5362 AVG Training Acc 75.3036% AVG Test Acc 74.8276%\n",
            "\n",
            "Fold 2\n",
            "Epoch:1/10 AVG Training Loss:0.5268 AVG Test Loss:0.5094 AVG Training Acc 75.4059% AVG Test Acc 76.092%\n",
            "Epoch:2/10 AVG Training Loss:0.5251 AVG Test Loss:0.5104 AVG Training Acc 75.2269% AVG Test Acc 75.7471%\n",
            "Epoch:3/10 AVG Training Loss:0.5241 AVG Test Loss:0.5081 AVG Training Acc 75.2269% AVG Test Acc 76.4368%\n",
            "Epoch:4/10 AVG Training Loss:0.5235 AVG Test Loss:0.5091 AVG Training Acc 75.5465% AVG Test Acc 75.977%\n",
            "Epoch:5/10 AVG Training Loss:0.5217 AVG Test Loss:0.5065 AVG Training Acc 75.3931% AVG Test Acc 76.3218%\n",
            "Epoch:6/10 AVG Training Loss:0.5217 AVG Test Loss:0.5073 AVG Training Acc 75.6232% AVG Test Acc 76.4368%\n",
            "Epoch:7/10 AVG Training Loss:0.521 AVG Test Loss:0.505 AVG Training Acc 75.4442% AVG Test Acc 76.6667%\n",
            "Epoch:8/10 AVG Training Loss:0.5198 AVG Test Loss:0.5116 AVG Training Acc 75.5465% AVG Test Acc 75.8621%\n",
            "Epoch:9/10 AVG Training Loss:0.5195 AVG Test Loss:0.5055 AVG Training Acc 75.4059% AVG Test Acc 76.4368%\n",
            "Epoch:10/10 AVG Training Loss:0.5191 AVG Test Loss:0.5054 AVG Training Acc 75.3803% AVG Test Acc 76.6667%\n",
            "\n",
            "Fold 3\n",
            "Epoch:1/10 AVG Training Loss:0.5167 AVG Test Loss:0.5278 AVG Training Acc 75.572% AVG Test Acc 74.8276%\n",
            "Epoch:2/10 AVG Training Loss:0.5162 AVG Test Loss:0.5276 AVG Training Acc 75.866% AVG Test Acc 75.6322%\n",
            "Epoch:3/10 AVG Training Loss:0.5155 AVG Test Loss:0.5262 AVG Training Acc 75.7893% AVG Test Acc 75.0575%\n",
            "Epoch:4/10 AVG Training Loss:0.5146 AVG Test Loss:0.5261 AVG Training Acc 75.8788% AVG Test Acc 75.2874%\n",
            "Epoch:5/10 AVG Training Loss:0.5152 AVG Test Loss:0.5281 AVG Training Acc 75.5592% AVG Test Acc 74.8276%\n",
            "Epoch:6/10 AVG Training Loss:0.5143 AVG Test Loss:0.5274 AVG Training Acc 75.7766% AVG Test Acc 74.8276%\n",
            "Epoch:7/10 AVG Training Loss:0.5138 AVG Test Loss:0.5251 AVG Training Acc 75.6999% AVG Test Acc 75.5172%\n",
            "Epoch:8/10 AVG Training Loss:0.5146 AVG Test Loss:0.525 AVG Training Acc 75.93% AVG Test Acc 75.7471%\n",
            "Epoch:9/10 AVG Training Loss:0.5132 AVG Test Loss:0.5248 AVG Training Acc 75.8405% AVG Test Acc 75.7471%\n",
            "Epoch:10/10 AVG Training Loss:0.5139 AVG Test Loss:0.5247 AVG Training Acc 75.8916% AVG Test Acc 75.1724%\n",
            "\n",
            "Fold 4\n",
            "Epoch:1/10 AVG Training Loss:0.5132 AVG Test Loss:0.5288 AVG Training Acc 75.9458% AVG Test Acc 74.2232%\n",
            "Epoch:2/10 AVG Training Loss:0.5126 AVG Test Loss:0.5316 AVG Training Acc 75.7413% AVG Test Acc 74.2232%\n",
            "Epoch:3/10 AVG Training Loss:0.5129 AVG Test Loss:0.5279 AVG Training Acc 75.6391% AVG Test Acc 74.3383%\n",
            "Epoch:4/10 AVG Training Loss:0.5129 AVG Test Loss:0.5254 AVG Training Acc 75.7797% AVG Test Acc 74.6835%\n",
            "Epoch:5/10 AVG Training Loss:0.512 AVG Test Loss:0.528 AVG Training Acc 76.0225% AVG Test Acc 74.5685%\n",
            "Epoch:6/10 AVG Training Loss:0.5123 AVG Test Loss:0.5263 AVG Training Acc 76.0353% AVG Test Acc 74.1082%\n",
            "Epoch:7/10 AVG Training Loss:0.5117 AVG Test Loss:0.5272 AVG Training Acc 76.0481% AVG Test Acc 74.4534%\n",
            "Epoch:8/10 AVG Training Loss:0.5119 AVG Test Loss:0.5262 AVG Training Acc 75.9969% AVG Test Acc 73.9931%\n",
            "Epoch:9/10 AVG Training Loss:0.5118 AVG Test Loss:0.5246 AVG Training Acc 76.0225% AVG Test Acc 74.6835%\n",
            "Epoch:10/10 AVG Training Loss:0.5112 AVG Test Loss:0.5246 AVG Training Acc 76.0353% AVG Test Acc 74.4534%\n",
            "\n",
            "Fold 5\n",
            "Epoch:1/10 AVG Training Loss:0.5111 AVG Test Loss:0.5261 AVG Training Acc 76.0992% AVG Test Acc 73.7629%\n",
            "Epoch:2/10 AVG Training Loss:0.5111 AVG Test Loss:0.525 AVG Training Acc 76.112% AVG Test Acc 74.5685%\n",
            "Epoch:3/10 AVG Training Loss:0.5112 AVG Test Loss:0.5255 AVG Training Acc 75.9202% AVG Test Acc 74.5685%\n",
            "Epoch:4/10 AVG Training Loss:0.5109 AVG Test Loss:0.5249 AVG Training Acc 76.1887% AVG Test Acc 74.1082%\n",
            "Epoch:5/10 AVG Training Loss:0.5106 AVG Test Loss:0.5297 AVG Training Acc 76.1887% AVG Test Acc 73.0725%\n",
            "Epoch:6/10 AVG Training Loss:0.5107 AVG Test Loss:0.5263 AVG Training Acc 76.4059% AVG Test Acc 74.7986%\n",
            "Epoch:7/10 AVG Training Loss:0.5107 AVG Test Loss:0.5248 AVG Training Acc 76.3676% AVG Test Acc 73.878%\n",
            "Epoch:8/10 AVG Training Loss:0.5101 AVG Test Loss:0.539 AVG Training Acc 76.3548% AVG Test Acc 72.382%\n",
            "Epoch:9/10 AVG Training Loss:0.5104 AVG Test Loss:0.5263 AVG Training Acc 76.0353% AVG Test Acc 74.6835%\n",
            "Epoch:10/10 AVG Training Loss:0.5103 AVG Test Loss:0.5304 AVG Training Acc 76.342% AVG Test Acc 74.6835%\n",
            "\n",
            "Fold 6\n",
            "Epoch:1/10 AVG Training Loss:0.5132 AVG Test Loss:0.5043 AVG Training Acc 75.8052% AVG Test Acc 77.2152%\n",
            "Epoch:2/10 AVG Training Loss:0.5126 AVG Test Loss:0.5026 AVG Training Acc 75.8052% AVG Test Acc 77.4453%\n",
            "Epoch:3/10 AVG Training Loss:0.5131 AVG Test Loss:0.5015 AVG Training Acc 75.6774% AVG Test Acc 78.1358%\n",
            "Epoch:4/10 AVG Training Loss:0.5127 AVG Test Loss:0.5029 AVG Training Acc 75.6774% AVG Test Acc 77.3303%\n",
            "Epoch:5/10 AVG Training Loss:0.5124 AVG Test Loss:0.5002 AVG Training Acc 75.7797% AVG Test Acc 77.9056%\n",
            "Epoch:6/10 AVG Training Loss:0.5126 AVG Test Loss:0.5006 AVG Training Acc 75.8947% AVG Test Acc 77.9056%\n",
            "Epoch:7/10 AVG Training Loss:0.5125 AVG Test Loss:0.5018 AVG Training Acc 75.5624% AVG Test Acc 78.1358%\n",
            "Epoch:8/10 AVG Training Loss:0.5125 AVG Test Loss:0.5001 AVG Training Acc 75.8436% AVG Test Acc 77.7906%\n",
            "Epoch:9/10 AVG Training Loss:0.5127 AVG Test Loss:0.5017 AVG Training Acc 75.8436% AVG Test Acc 77.3303%\n",
            "Epoch:10/10 AVG Training Loss:0.5117 AVG Test Loss:0.5024 AVG Training Acc 75.7285% AVG Test Acc 78.2509%\n",
            "\n",
            "Fold 7\n",
            "Epoch:1/10 AVG Training Loss:0.5105 AVG Test Loss:0.5122 AVG Training Acc 75.8436% AVG Test Acc 75.8343%\n",
            "Epoch:2/10 AVG Training Loss:0.5106 AVG Test Loss:0.5127 AVG Training Acc 75.8308% AVG Test Acc 76.4097%\n",
            "Epoch:3/10 AVG Training Loss:0.5103 AVG Test Loss:0.5148 AVG Training Acc 76.0992% AVG Test Acc 75.0288%\n",
            "Epoch:4/10 AVG Training Loss:0.5107 AVG Test Loss:0.5126 AVG Training Acc 76.0481% AVG Test Acc 76.5247%\n",
            "Epoch:5/10 AVG Training Loss:0.5107 AVG Test Loss:0.5123 AVG Training Acc 76.112% AVG Test Acc 75.7192%\n",
            "Epoch:6/10 AVG Training Loss:0.5102 AVG Test Loss:0.5128 AVG Training Acc 75.9075% AVG Test Acc 76.4097%\n",
            "Epoch:7/10 AVG Training Loss:0.5101 AVG Test Loss:0.5134 AVG Training Acc 76.0225% AVG Test Acc 76.1795%\n",
            "Epoch:8/10 AVG Training Loss:0.5102 AVG Test Loss:0.5162 AVG Training Acc 76.0992% AVG Test Acc 74.6835%\n",
            "Epoch:9/10 AVG Training Loss:0.5098 AVG Test Loss:0.5157 AVG Training Acc 75.9842% AVG Test Acc 76.1795%\n",
            "Epoch:10/10 AVG Training Loss:0.5106 AVG Test Loss:0.5187 AVG Training Acc 76.1631% AVG Test Acc 76.0644%\n",
            "\n",
            "Fold 8\n",
            "Epoch:1/10 AVG Training Loss:0.5121 AVG Test Loss:0.5015 AVG Training Acc 75.8563% AVG Test Acc 76.5247%\n",
            "Epoch:2/10 AVG Training Loss:0.512 AVG Test Loss:0.4953 AVG Training Acc 75.9586% AVG Test Acc 77.4453%\n",
            "Epoch:3/10 AVG Training Loss:0.5118 AVG Test Loss:0.4968 AVG Training Acc 75.8052% AVG Test Acc 76.1795%\n",
            "Epoch:4/10 AVG Training Loss:0.5125 AVG Test Loss:0.4939 AVG Training Acc 75.9202% AVG Test Acc 77.3303%\n",
            "Epoch:5/10 AVG Training Loss:0.5122 AVG Test Loss:0.494 AVG Training Acc 75.8691% AVG Test Acc 77.7906%\n",
            "Epoch:6/10 AVG Training Loss:0.5121 AVG Test Loss:0.4954 AVG Training Acc 75.8436% AVG Test Acc 77.3303%\n",
            "Epoch:7/10 AVG Training Loss:0.5112 AVG Test Loss:0.4939 AVG Training Acc 75.6135% AVG Test Acc 77.5604%\n",
            "Epoch:8/10 AVG Training Loss:0.512 AVG Test Loss:0.4944 AVG Training Acc 75.7541% AVG Test Acc 77.4453%\n",
            "Epoch:9/10 AVG Training Loss:0.5114 AVG Test Loss:0.5048 AVG Training Acc 75.9586% AVG Test Acc 76.6398%\n",
            "Epoch:10/10 AVG Training Loss:0.5114 AVG Test Loss:0.4957 AVG Training Acc 75.8819% AVG Test Acc 76.87%\n",
            "\n",
            "Fold 9\n",
            "Epoch:1/10 AVG Training Loss:0.5101 AVG Test Loss:0.5127 AVG Training Acc 75.8563% AVG Test Acc 76.4097%\n",
            "Epoch:2/10 AVG Training Loss:0.51 AVG Test Loss:0.5128 AVG Training Acc 75.8819% AVG Test Acc 75.8343%\n",
            "Epoch:3/10 AVG Training Loss:0.5096 AVG Test Loss:0.5129 AVG Training Acc 75.8052% AVG Test Acc 76.1795%\n",
            "Epoch:4/10 AVG Training Loss:0.5099 AVG Test Loss:0.5128 AVG Training Acc 75.9714% AVG Test Acc 76.4097%\n",
            "Epoch:5/10 AVG Training Loss:0.5097 AVG Test Loss:0.5154 AVG Training Acc 76.0992% AVG Test Acc 76.0644%\n",
            "Epoch:6/10 AVG Training Loss:0.5095 AVG Test Loss:0.5199 AVG Training Acc 76.2142% AVG Test Acc 75.2589%\n",
            "Epoch:7/10 AVG Training Loss:0.5102 AVG Test Loss:0.5159 AVG Training Acc 75.9458% AVG Test Acc 76.0644%\n",
            "Epoch:8/10 AVG Training Loss:0.5097 AVG Test Loss:0.5132 AVG Training Acc 76.0097% AVG Test Acc 76.1795%\n",
            "Epoch:9/10 AVG Training Loss:0.509 AVG Test Loss:0.5141 AVG Training Acc 76.0736% AVG Test Acc 75.6041%\n",
            "Epoch:10/10 AVG Training Loss:0.5093 AVG Test Loss:0.5146 AVG Training Acc 76.0097% AVG Test Acc 75.9494%\n",
            "\n",
            "Fold 10\n",
            "Epoch:1/10 AVG Training Loss:0.5118 AVG Test Loss:0.4867 AVG Training Acc 75.8436% AVG Test Acc 77.2152%\n",
            "Epoch:2/10 AVG Training Loss:0.5124 AVG Test Loss:0.4874 AVG Training Acc 75.6518% AVG Test Acc 78.2509%\n",
            "Epoch:3/10 AVG Training Loss:0.5123 AVG Test Loss:0.4879 AVG Training Acc 75.8691% AVG Test Acc 76.87%\n",
            "Epoch:4/10 AVG Training Loss:0.5122 AVG Test Loss:0.4927 AVG Training Acc 75.8436% AVG Test Acc 77.6755%\n",
            "Epoch:5/10 AVG Training Loss:0.5124 AVG Test Loss:0.4983 AVG Training Acc 75.6391% AVG Test Acc 77.5604%\n",
            "Epoch:6/10 AVG Training Loss:0.5124 AVG Test Loss:0.4978 AVG Training Acc 75.6902% AVG Test Acc 77.5604%\n",
            "Epoch:7/10 AVG Training Loss:0.5126 AVG Test Loss:0.4884 AVG Training Acc 75.7157% AVG Test Acc 78.0207%\n",
            "Epoch:8/10 AVG Training Loss:0.5114 AVG Test Loss:0.4958 AVG Training Acc 75.9969% AVG Test Acc 77.7906%\n",
            "Epoch:9/10 AVG Training Loss:0.5126 AVG Test Loss:0.4879 AVG Training Acc 75.8691% AVG Test Acc 77.2152%\n",
            "Epoch:10/10 AVG Training Loss:0.5119 AVG Test Loss:0.4876 AVG Training Acc 75.8308% AVG Test Acc 77.6755%\n",
            "\n",
            "Performance of 10 fold cross validation\n",
            "Average Training Loss: 0.5135 \t Average Test Loss: 0.514 \t Average Training Acc: 75.8567 \t Average Test Acc: 76.0614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save results"
      ],
      "metadata": {
        "id": "mKn3DJIiU5J5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "data.append(['PyTorch LogisticRegression', 'OHE', acc_ohe, 0, \"---\"])\n",
        "data.append(['PyTorch LogisticRegression', 'TE', acc_te, 0, \"---\"])\n",
        "data.append(['Pytorch LogisticRegression', 'LOOE', acc_looe, 0, \"---\"])"
      ],
      "metadata": {
        "id": "LpDdzBjAU0mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from os.path import exists\n",
        "resfile = 'spaceship_results.csv'"
      ],
      "metadata": {
        "id": "bSghifBEU5sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if exists(resfile):\n",
        "  f = open(resfile, 'a')\n",
        "  writer = csv.writer(f)\n",
        "else:\n",
        "  header = ['Model', 'Categories_encoding', 'Initial_accuracy', 'Tuned_Accuracy', 'Important_Features']\n",
        "  f = open(resfile, 'w', newline='')\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow(header)\n",
        "\n",
        "writer.writerows(data)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "ky2YMX6DVU6P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}